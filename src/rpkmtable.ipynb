{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metagenomic RPKM estimator\n",
    "\n",
    "# Author: Jakob Nybo Nissen, DTU Bioinformatics, jakni@bioinformatics.dtu.dk\n",
    "# Date: 2018-04-24\n",
    "    \n",
    "# This script calculates RPKM, when paired end reads are mapped to a contig\n",
    "# catalogue with BWA MEM. It will not be accurate with single end reads or\n",
    "# any other mapper than BWA MEM.\n",
    "\n",
    "# Theory:\n",
    "# We want a simple way to estimate abundance of redundant contig catalogues.\n",
    "# Earlier we used to run analysis on deduplicated gene catalogues, but since \n",
    "# both depth and kmer composition are only stable for longer contigs, we have\n",
    "# moved to contig catalogues. We have not found a way of deduplicating contigs.\n",
    "\n",
    "# For this we have until now used two methods:\n",
    "# 1) Only counting the primary hits. In this case the read will never be\n",
    "# assigned to any contig which differ by just 1 basepair. Even for\n",
    "# identical contigs, reads are assigned randomly which causes noise.\n",
    "\n",
    "# 2) Using MetaBAT's jgi_summarize_bam_contig_depths, a script which is not\n",
    "# documented and we cannot figure out how works. When testing with small\n",
    "# toy data, it produces absurd results.\n",
    "\n",
    "# This script is an attempt to take an approach as simple as possible while\n",
    "# still being sound technically. We simply count the number of reads in a\n",
    "# contig normalized by contig length and total number of reads.\n",
    "\n",
    "# We look at all hits, including secondary hits. We do not discount partial\n",
    "# alignments. Also, if a read maps to multiple contigs, we don't count each hit\n",
    "# as less than if it mapped to both. The reason for all these decisions is that\n",
    "# if the aligner believes it's a hit, we believe the contig is present.\n",
    "\n",
    "# We do not take varying insert sizes into account. It is unlikely that\n",
    "# any contig with enough reads to provide a reliable estimate of depth would,\n",
    "# by chance, only recruit read pairs with short or long insert size. So this\n",
    "# will average out over all contigs.\n",
    "\n",
    "# We DO filter the input file for duplicate lines, as BWA MEM erroneously\n",
    "# produces quite a few of them.\n",
    "\n",
    "# We count each read independently, because BWA MEM often assigns mating reads\n",
    "# to different contigs. If a read has their mate unmapped, we count it twice\n",
    "# to compensate (one single read corresponds to two paired reads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metagenomic depth estimator\\n\\nAuthor: Jakob Nybo Nissen, DTU Bioinformatics, jakni@bioinformatics.dtu.dk\\n    \\nThis script calculates RPKM, when paired end reads are mapped to a contig\\ncatalogue with BWA MEM. Because of the idiosyncracities of BWA, it will not\\nbe accurate with single end reads or any other mapper than BWA MEM.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__doc__ = \"\"\"Metagenomic RPKM estimator\n",
    "\n",
    "Author: Jakob Nybo Nissen, DTU Bioinformatics, jakni@bioinformatics.dtu.dk\n",
    "    \n",
    "This script calculates reads per kilobase per million mapped reads (RPKM),\n",
    "when paired end reads are mapped to a contig catalogue with BWA MEM.\n",
    "Because of the idiosyncracities of BWA, it will not be accurate with single end\n",
    "reads or any other mapper than BWA MEM.\n",
    "\n",
    "Requires the module pysam to run.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_references(alignedsegment):\n",
    "    \"\"\"Given a pysam aligned segment, returns a list with the names of all\n",
    "    references the read maps to, both primary and secondary hits.\n",
    "    \"\"\"\n",
    "    \n",
    "    references = [alignedsegment.reference_name]\n",
    "    \n",
    "    # Some reads don't have secondary hits\n",
    "    if not alignedsegment.has_tag('XA'):\n",
    "        return references\n",
    "    \n",
    "    # XA is a string contigname1,<other info>;contigname2,<other info>; ...\n",
    "    secondary_alignment_string = alignedsegment.get_tag('XA')\n",
    "    secondary_alignments = secondary_alignment_string.split(';')[:-1]\n",
    "    \n",
    "    for secondary_alignment in secondary_alignments:\n",
    "        references.append(secondary_alignment.partition(',')[0])\n",
    "        \n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_segments(segmentiterator, minscore):\n",
    "    \"\"\"Returns an iterator of AlignedSegment filtered for reads with low\n",
    "    alignment score, and for any segments identical to the previous segment.\n",
    "    This is necessary as BWA MEM produces dopplegangers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First get the first segment, so we in the loop can compare to the previous\n",
    "    alignedsegment = next(segmentiterator)\n",
    "    \n",
    "    yield alignedsegment\n",
    "        \n",
    "    lastname = alignedsegment.query_name\n",
    "    lastwasforward = alignedsegment.flag & 64 == 64\n",
    "        \n",
    "    for alignedsegment in segmentiterator:\n",
    "        if alignedsegment.get_tag('AS') < minscore:\n",
    "            continue\n",
    "        \n",
    "        # Depressingly, BWA somtimes outputs the same read multiple times.\n",
    "        # We identify them by having same name as directionality as previous. \n",
    "        thisname = alignedsegment.query_name\n",
    "        thisisforward = alignedsegment.flag & 64 == 64\n",
    "        \n",
    "        if thisisforward is not lastwasforward or thisname != lastname:\n",
    "            yield alignedsegment\n",
    "            \n",
    "            lastname = thisname\n",
    "            lastwasforward = thisisforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contig_rpkms(identifier, path, minscore):\n",
    "    \"\"\"Returns  RPKM (reads per kilobase per million mapped reads)\n",
    "    for all contigs present in BAM header.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Parsing file:', path)\n",
    "    \n",
    "    bamfile = pysam.AlignmentFile(path, \"rb\")\n",
    "    \n",
    "    # We can only get secondary alignment reference names, not indices. So we must\n",
    "    # make an idof dict to look up the indices.\n",
    "    idof = {contig: i for i, contig in enumerate(bamfile.references)}\n",
    "    contiglengths = bamfile.lengths\n",
    "    halfreads = [0] * len(contiglengths)\n",
    "    \n",
    "    nhalfreads = 0\n",
    "    for segment in filter_segments(bamfile, minscore):\n",
    "        nhalfreads += 1\n",
    "        \n",
    "        # Read w. unmapped mates count twice as they represent a whole read\n",
    "        value = 2 if segment.mate_is_unmapped else 1\n",
    "        \n",
    "        for reference in get_all_references(segment):\n",
    "            id = idof[reference]\n",
    "            halfreads[id] += value\n",
    "            \n",
    "    bamfile.close()\n",
    "    \n",
    "    print('Done parsing file:', path)\n",
    "    \n",
    "    rpkms = list()\n",
    "    \n",
    "    # Compensate for having paired reads\n",
    "    millionmappedreads = nhalfreads / 2e6\n",
    "    \n",
    "    for contiglength, nhalfreads in zip(contiglengths, halfreads):\n",
    "        kilobases = contiglength / 1000\n",
    "        rpkms.append(nhalfreads / (kilobases * millionmappedreads))\n",
    "    \n",
    "    return identifier, rpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(paths, minscore, outfile, is_matrix):\n",
    "    \"\"\"Spawns processes to parse BAM files and get contig rpkms from them,\n",
    "    then prints the resulting table to an output file\"\"\"\n",
    "    \n",
    "    # Get references and lengths from first BAM file.\n",
    "    # We need these to print them in the output.\n",
    "    # Might as well do it before spawning all those processes.\n",
    "    firstfile = pysam.AlignmentFile(paths[0], \"rb\")\n",
    "    references = firstfile.references\n",
    "    lengths = firstfile.lengths\n",
    "    \n",
    "    if not len(references) == len(lengths):\n",
    "        raise ValueError('Could not parse headers of first bam-file')\n",
    "    \n",
    "    # Spawn independent processed to calculate RPKM for each of the BAM files\n",
    "    processresults = list()\n",
    "    processes_done = 0\n",
    "    \n",
    "    # This is just to print to terminal when a process finishes. Not necessary.\n",
    "    def callback(result, totalps=len(paths)):\n",
    "        \"Generator yielding processed\"\n",
    "        nonlocal processes_done\n",
    "        processes_done += 1\n",
    "        print('Files processed: {}/{}'.format(processes_done, totalps))\n",
    "        return None\n",
    "\n",
    "    # Queue all the processes\n",
    "    with multiprocessing.Pool(processes=args.processors) as pool:\n",
    "        for fileno, path in enumerate(paths):\n",
    "            arguments = (fileno, path, args.minscore)\n",
    "            processresults.append(pool.apply_async(get_contig_rpkms, arguments,\n",
    "                                                  callback=callback, error_callback=callback))\n",
    "        \n",
    "        # For some reason, this is needed.\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "            \n",
    "    print('All processes finished. Checking outputs')\n",
    "    sample_rpkms = list()\n",
    "    \n",
    "    for processresult in processresults:\n",
    "        if processresult.successful():\n",
    "            sample_rpkms.append(processresult.get())\n",
    "            \n",
    "        else:\n",
    "            raise multiprocessing.ProcessError\n",
    "    \n",
    "    # sample_rpkms now contain (identifier, sample_rpkms) tuples, in the order\n",
    "    # they were returned from the pool. We want to sort them by identifier,\n",
    "    # so that we know which RPKMs belong to which BAM file\n",
    "    sample_rpkms.sort()\n",
    "    \n",
    "    # Now we can discard the identifiers\n",
    "    sample_rpkms = [i[1] for i in sample_rpkms]\n",
    "    \n",
    "    # Each BAM file MUST contain the same headers\n",
    "    if not all(len(rpkms) == len(lengths) for rpkms in sample_rpkms):\n",
    "        raise ValueError('Not all BAM files contain the same amount of headers.')\n",
    "    \n",
    "    print('Outputs alright. Printing table.')\n",
    "    \n",
    "    with open(outfile, 'w') as filehandle:\n",
    "        # Print header if asked\n",
    "        if not is_matrix:\n",
    "            print('#contig\\tcontiglength', '\\t'.join(paths), sep='\\t', file=filehandle)\n",
    "    \n",
    "        # Print the actual output\n",
    "        for fields in zip(references, lengths, *sample_rpkms):\n",
    "            numbers = '\\t'.join([str(round(i, 3)) for i in fields[2:]])\n",
    "            \n",
    "            if not is_matrix:\n",
    "                print(fields[0], fields[1], sep='\\t', end='\\t', file=filehandle)\n",
    "                \n",
    "            print(numbers, file=filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    usage = \"python rpkmtable.py [OPTION ...] OUTFILE BAMFILE [BAMFILE ...]\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        usage=usage)\n",
    "    \n",
    "    # Get the maximal number of extra processes to spawn\n",
    "    cpus = os.cpu_count()\n",
    "    \n",
    "    # Positional arguments\n",
    "    parser.add_argument('outfile', help='path to output file')\n",
    "    parser.add_argument('bamfiles', help='relative path to find bam files', nargs='+')\n",
    "    \n",
    "    # Optional arguments\n",
    "    parser.add_argument('-m', dest='minscore', type=int, default=50,\n",
    "                        help='minimum alignment score [50]')\n",
    "    parser.add_argument('-p', dest='processors', type=int, default=cpus,\n",
    "                        help=('number of extra processes to spawn '\n",
    "                              '[min(' + str(cpus) + ', nfiles)]'))\n",
    "    parser.add_argument('--matrix', action='store_true',\n",
    "        help='only print RPKM, not headers, contignames or lengths')\n",
    "    \n",
    "    # If no arguments, print help\n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        sys.exit()\n",
    "        \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Check number of cores\n",
    "    if args.processors < 1:\n",
    "        raise argsparse.ArgumentTypeError('Zero or negative processors provided.')\n",
    "        \n",
    "    # Check presence of output file\n",
    "    if os.path.exists(args.outfile):\n",
    "        raise FileExistsError('Output file: ' + args.outfile)\n",
    "        \n",
    "    # Check presence of all BAM files\n",
    "    for bamfile in args.bamfiles:\n",
    "        if not os.path.isfile(bamfile):\n",
    "            raise FileNotFoundError('Bam file: ' + bamfile)\n",
    "            \n",
    "    # If more CPUs than files, scale down CPUs\n",
    "    if args.processors > len(args.bamfiles):\n",
    "        args.processors = len(args.bamfiles)\n",
    "        print('More processes given than files, scaling to {}.'.format(args.processors))\n",
    "    \n",
    "        print('Number of files:', len(args.bamfiles))\n",
    "    print('Number of parallel processes:', args.processors)\n",
    "    \n",
    "    # Warn user if --matrix is given\n",
    "    if args.matrix:\n",
    "        print('No headers given. Columns ordered like file arguments:')\n",
    "        for n, bamfile in enumerate(args.bamfiles):\n",
    "            print('\\tcol ', str(n+1), ': ', bamfile, sep='')\n",
    "    \n",
    "    starttime = time.time()\n",
    "    print('Starting RPKM estimation.')\n",
    "    \n",
    "    # Run the stuff!\n",
    "    main(args.bamfiles, args.minscore, args.outfile, args.matrix)\n",
    "    \n",
    "    elapsed = round(time.time() - starttime)\n",
    "    print('Done in {} seconds.'.format(elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
